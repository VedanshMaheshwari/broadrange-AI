{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing required libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import openai\n",
    "openai.api_key = api_key = \"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\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T12:09:16.819096600Z",
     "start_time": "2024-01-20T12:09:16.811749800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All required functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def can_crawl(url, user_agent=\"MyBot\"):\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(url, \"/robots.txt\"))\n",
    "    rp.read()\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "def get_image_urls_from_website(url):\n",
    "    if not can_crawl(url):\n",
    "        print(f\"Crawling not allowed for {url}. Check robots.txt.\")\n",
    "        return []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        img_tags = soup.find_all('img')\n",
    "        # is_valid_image = lambda url: os.path.splitext(url)[1].lower() in ('.jpg', '.jpeg', '.png')\n",
    "        is_valid_image = lambda url : True\n",
    "        image_urls = [urljoin(url, img['src']) for img in img_tags if is_valid_image(img['src'])]\n",
    "        return image_urls\n",
    "\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def store_urls_in_spreadsheet(image_urls, output_csv=\"image_urls.csv\"):\n",
    "    data = [{'Image URL {}'.format(i + 1): url for i, url in enumerate(row)} for row in image_urls]\n",
    "    df = pd.DataFrame(data)\n",
    "    df.T.to_csv(output_csv, index=False)\n",
    "    print(f\"Image URLs saved to {output_csv}\")\n",
    "\n",
    "def extract_description(img_url,api_key):\n",
    "    def encode_image(image_url):\n",
    "        try:\n",
    "            # Add the appropriate scheme to the relative URL\n",
    "            full_url = urljoin(\"https:\", image_url)\n",
    "\n",
    "            response = requests.get(full_url)\n",
    "            response.raise_for_status()  # Raise an exception for bad responses (e.g., 404)\n",
    "\n",
    "            return base64.b64encode(response.content).decode('utf-8')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching image from '{full_url}': {e}\")\n",
    "            return None  # Handle the error gracefully, e.g., return None or a default value\n",
    "    # Replace \"YOUR_API_KEY\" with the actual API key\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(img_url)\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Whatâ€™s in this image?\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "def generate_image_url(prompt):\n",
    "    response = openai.Image.create(\n",
    "        model = 'dall-e-3',\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    return response.data[0].url\n",
    "\n",
    "def generate_better_version(img_url,api_key):\n",
    "    return generate_image_url(extract_description(img_url,api_key)['choices'][0]['message']['content'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:52:35.832431400Z",
     "start_time": "2024-01-20T13:52:35.809404600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The functional part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs saved to image_urls.csv\n"
     ]
    }
   ],
   "source": [
    "website_url = input(\"Enter link:\")\n",
    "image_url = get_image_urls_from_website(website_url)\n",
    "new_urls = []\n",
    "for url in image_url:\n",
    "    new_urls.append(generate_better_version(url,api_key))\n",
    "store_urls_in_spreadsheet([image_url,new_urls])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:54:39.893541600Z",
     "start_time": "2024-01-20T13:52:49.395110600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_image_urls_in_file(html_file_path, csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    for img_tag in img_tags:\n",
    "        old_url = img_tag.get('src', '')\n",
    "        new_url = df[df['Old Image URL'] == old_url]['New Image URL'].values\n",
    "        if new_url:\n",
    "            img_tag['src'] = new_url[0]\n",
    "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(soup))\n",
    "replace_image_urls_in_file('path/to/your/mapping.csv', 'path/to/your/mapping.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```import requests\n",
    "from urllib.parse import urlparse, unquote\n",
    "def save_image_locally(image_url, local_filename=None):\n",
    "    response = requests.get(image_url, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract the filename from the URL or use a provided local filename\n",
    "        if local_filename is None:\n",
    "            parsed_url = urlparse(image_url)\n",
    "            local_filename = unquote(os.path.basename(parsed_url.path))\n",
    "\n",
    "        # Save the image to a local file\n",
    "        with open(local_filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"Image saved locally as: {local_filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download the image. Status code: {response.status_code}\")\n",
    "save_image_locally(website_url)```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded successfully to C:\\Coding\\Python\\BroadrangeAI\\BagduBigdu\\image.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Image downloaded successfully to {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "\n",
    "# Example usage:\n",
    "image_url = \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7SvBjzL7fdLiD1CtAVvDwkIN/user-1WZMuMqlV2ZYTPCwiw34OOtV/img-oqQByf4L5PnK8EMGLY5wRKDU.png?st=2024-01-20T12%3A53%3A29Z&se=2024-01-20T14%3A53%3A29Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-01-19T19%3A35%3A17Z&ske=2024-01-20T19%3A35%3A17Z&sks=b&skv=2021-08-06&sig=FDH%2B%2BKOijTCe%2BPlvq%2BeJ%2B959ODqEzlvZI%2BYKJ9X9ECE%3D\"\n",
    "save_path = os.path.join(os.getcwd(), \"BagduBigdu\", \"image.png\")\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "download_image(image_url, save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T14:15:07.919458Z",
     "start_time": "2024-01-20T14:15:02.409973200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
