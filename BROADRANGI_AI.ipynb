{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34751e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.amazon.in/\"\n",
    "url\n",
    "\n",
    "def fetch_web_content_with_coordinates(url):\n",
    "    chrome_driver_path = 'C:/Users/sriha/REQD PROGS/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "    service = Service(chrome_driver_path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    data = []\n",
    "    \n",
    "    for index, img_tag in enumerate(img_tags):\n",
    "        x, y = get_coordinates_using_js_or_style(driver, img_tag)\n",
    "\n",
    "        parent_div = img_tag.find_parent('div')\n",
    "        sku = parent_div.find('span', class_='sku')\n",
    "        \n",
    "        img_url = img_tag[\"src\"]\n",
    "        \n",
    "        description = extract_description(img_url)\n",
    "\n",
    "        data.append({\n",
    "            \"img_url\": img_url,\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"sku\": sku.get_text() if sku else None,\n",
    "            \"description\": description\n",
    "        })\n",
    "\n",
    "    driver.quit()\n",
    "    return data\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "#add function called extract description\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "    \n",
    "def get_coordinates_using_js_or_style(driver, img_tag):\n",
    "    \"\"\"\n",
    "    Get image coordinates using JavaScript or from the style attribute.\n",
    "    \"\"\"\n",
    "    style = img_tag.get('style', '')\n",
    "\n",
    "    if 'left' in style and 'top' in style:\n",
    "        # Extract coordinates from the style attribute\n",
    "        x, y = map(int, [value.replace('px', '').strip() for value in style.split(';') if 'left' in value or 'top' in value])\n",
    "    else:\n",
    "        # If style doesn't contain valid position information, use JavaScript\n",
    "        x, y = get_coordinates_using_js(driver, img_tag)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def get_coordinates_using_js(driver, img_tag):\n",
    "    img_src = img_tag.get('src')\n",
    "\n",
    "    # Custom wait loop to handle delays in loading\n",
    "    max_wait_time = 20  # Maximum wait time in seconds\n",
    "    interval = 1  # Check interval in seconds\n",
    "    max_attempts = 5  # Maximum number of attempts\n",
    "    attempts = 0  # Counter for attempts\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            print(f\"Attempting to find image with src '{img_src}' (Attempt {attempts + 1})...\")\n",
    "            # Attempt to find the image element\n",
    "            img_element = driver.find_element(By.XPATH, f'//img[@src=\"{img_src}\"]')\n",
    "\n",
    "            # If found, get coordinates and break the loop\n",
    "            x = driver.execute_script('return arguments[0].getBoundingClientRect().left;', img_element)\n",
    "            y = driver.execute_script('return arguments[0].getBoundingClientRect().top;', img_element)\n",
    "            print(f\"Image with src '{img_src}' found. Coordinates: {x}, {y}\")\n",
    "            return float(x), float(y)\n",
    "        except NoSuchElementException:\n",
    "            # If not found, sleep for the interval and update attempts\n",
    "            time.sleep(interval)\n",
    "            attempts += 1\n",
    "\n",
    "    # If the maximum number of attempts is reached, print a message and return default coordinates\n",
    "    print(f\"Image with src '{img_src}' not found after {max_attempts} attempts. Using default coordinates.\")\n",
    "    return 0, 0  # Default coordinates\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def store_data_in_excel(data, filename=\"output.xlsx\"):\n",
    "    \"\"\"\n",
    "    Store the extracted data in an Excel file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    try:\n",
    "        # Code that may raise an exception\n",
    "        df.to_excel(\"C:/Users/sriha/Jupyter Notebooks/output.xlsx\", index=False)\n",
    "        print(\"Excel file successfully created.\")\n",
    "    except Exception as e:\n",
    "        # Handle the exception\n",
    "        print(f\"Error: {e}\")\n",
    "#     df = pd.DataFrame(data)\n",
    "#     df.to_excel(\"C:/Users/sriha/Jupyter Notebooks/output.xlsx\",index=False)\n",
    "\n",
    "def create_image_library_from_text(text_list):\n",
    "    \"\"\"\n",
    "    Create a digital image library using DALL·E from the extracted text.\n",
    "    \"\"\"\n",
    "    image_library = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        if text:\n",
    "            # Generate images using DALL·E (this is a hypothetical call)\n",
    "            images = text2im(prompts=[text])\n",
    "            image_library.extend(images)\n",
    "    \n",
    "    return image_library\n",
    "\n",
    "def main(url):\n",
    "    extracted_data = fetch_web_content_with_coordinates(url)\n",
    "    \n",
    "    # Check if there is data before proceeding\n",
    "    if extracted_data:\n",
    "        # Store the data in an Excel file\n",
    "        store_data_in_excel(extracted_data)\n",
    "        \n",
    "        # Create image library from extracted text\n",
    "        extracted_texts = [item.get('description', '') for item in extracted_data]\n",
    "        image_library = create_image_library_from_text(extracted_texts)\n",
    "\n",
    "        # Save the images from the image library\n",
    "        for index, image in enumerate(image_library):\n",
    "            image.save(f\"image_library_{index}.png\")\n",
    "    else:\n",
    "        print(\"No data extracted from the web content.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter the website URL: \")\n",
    "    url_to_crawl = url\n",
    "    user_agent_name = input(\"enter the name of ur crawler\")\n",
    "    is_allowed_to_crawl(url, user_agent=user_agent_name)\n",
    "    main(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE BLOCK TO INSERT THE ROBOT.TXT CHECKER AT THE START OF THE PROGRAM TO SEE IF WEB CRAWLING IS ALLOWED\n",
    "import urllib.robotparser\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def is_allowed_to_crawl(url, user_agent=\"my_crawler\"):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(urljoin(url, '/robots.txt'))\n",
    "    rp.read()\n",
    "\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "if is_allowed_to_crawl(url_to_crawl, user_agent_name):\n",
    "    print(f\"The crawler is allowed to crawl {url_to_crawl}\")\n",
    "    # Your crawling logic goes here\n",
    "else:\n",
    "    print(f\"The crawler is not allowed to crawl {url_to_crawl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a22c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description(img_url):\n",
    "    # Function to encode the image\n",
    "    def encode_image(image_url):\n",
    "        try:\n",
    "            # Add the appropriate scheme to the relative URL\n",
    "            full_url = urljoin(\"https:\", image_url)\n",
    "\n",
    "            response = requests.get(full_url)\n",
    "            response.raise_for_status()  # Raise an exception for bad responses (e.g., 404)\n",
    "\n",
    "            return base64.b64encode(response.content).decode('utf-8')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching image from '{full_url}': {e}\")\n",
    "            return None  # Handle the error gracefully, e.g., return None or a default value\n",
    "\n",
    "    # Replace \"YOUR_API_KEY\" with the actual API key\n",
    "    api_key = \"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\"\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(img_url)\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"What’s in this image?\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d4168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai_api_key = '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'\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Read the Excel file\n",
    "excel_file_path = 'C:/Users/sriha/Jupyter Notebooks/output.xlsx'\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Function to extract content from description\n",
    "def extract_content(description):\n",
    "    start_index = description.find('\"') + 1\n",
    "    end_index = description.find('\"', start_index)\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return description[start_index:end_index]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to generate image URL using OpenAI API\n",
    "def generate_image_url(prompt):\n",
    "    response = openai.Image.create(\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    return response.data[0].url\n",
    "\n",
    "# Process each row in the DataFrame\n",
    "image_urls = []\n",
    "for index, row in df.iterrows():\n",
    "    description = row['description']\n",
    "    content_segment = extract_content(description)\n",
    "    \n",
    "    # Only consider if content is found\n",
    "    if content_segment:\n",
    "        # Generate image URL using content segment\n",
    "        prompt = f\"Generate an image based on the following content: {content_segment}\"\n",
    "        image_url = generate_image_url(prompt)\n",
    "        image_urls.append(image_url)\n",
    "    else:\n",
    "        image_urls.append(\"\")\n",
    "\n",
    "# Add the generated image URLs to the DataFrame\n",
    "df['image_url'] = image_urls\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_excel_path = 'C:/Users/sriha/Jupyter Notebooks/output2.xlsx'\n",
    "df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been added to the DataFrame and saved to {output_excel_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
